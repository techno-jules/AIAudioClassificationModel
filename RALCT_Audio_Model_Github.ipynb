{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "RALCT Audio Model - Github.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheClassicTechno/audioclassmodel/blob/main/RALCT_Audio_Model_Github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction"
      ],
      "metadata": {
        "id": "mT_WTlr2BvCH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo8rX1KOYqOO"
      },
      "source": [
        "Welcome!\n",
        "Randomized Audiomentational Layered Convolutional Transformers (RALCT): \n",
        "A Novel Deep Learning Model for Environmental Sound Recognition\n",
        "\n",
        "RALCT's consistent classification accuracy: 93-95% <br>\n",
        "best validation accuracy so far: l_accuracy: 94.56% with learning rate 1e-3 and batch size 32 <br>\n",
        "human accuracy: (81.3%) \n",
        "\n",
        "Techniques: MFCCS+spectrogram layers, random augmentations, combine transformer and cnn models together\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS23tmDf8Tlf"
      },
      "source": [
        "#Import Dataset, Files, Libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oANTELdekUW0"
      },
      "source": [
        "! pip install kaggle\n",
        "!mkdir ~/.kaggle\n",
        "#!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "! kaggle datasets download chrisfilo/urbansound8k  \n",
        "#! kaggle datasets download chrisfilo/urbansound8k  --force\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download chrisfilo/urbansound8k\n",
        "! unzip urbansound8k"
      ],
      "metadata": {
        "id": "rC1q2jYOu_RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l3XV8M6fFJa",
        "execution": {
          "iopub.status.busy": "2021-11-18T21:41:15.390613Z",
          "iopub.execute_input": "2021-11-18T21:41:15.391076Z",
          "iopub.status.idle": "2021-11-18T21:41:18.290209Z",
          "shell.execute_reply.started": "2021-11-18T21:41:15.390955Z",
          "shell.execute_reply": "2021-11-18T21:41:18.289591Z"
        },
        "trusted": true
      },
      "source": [
        "import torch\n",
        "from scipy.io.wavfile import write\n",
        "from IPython.display import Audio\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os \n",
        "import matplotlib.pyplot as plt \n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm \n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import librosa.display\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization, Input\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing "
      ],
      "metadata": {
        "id": "CGdIpml6fJBy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xwsj58lTfFJm"
      },
      "source": [
        "### The Dataset: Audio Augmentations Method"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "seconds=5\n",
        "\n",
        "WINDOW_SIZE=2048 #portion u look at when transforming audio, a power of 2 as value\n",
        "HOP_SIZE=441 #10 milisec jump in time, default hop length is 512 #quarter hop, overlap four dif windows\n",
        "SAMPLING_FREQUENCY=44_100\n",
        "SAMPLE_LENGTH=seconds*SAMPLING_FREQUENCY #most files are 4 secs long\n",
        "\n",
        "N_MELS=128\n",
        "NOISE_CHANCE=0.2\n",
        "PITCH_CHANCE=0.2\n",
        "SPEED_CHANCE =0.2\n",
        "NOISE_FACTOR=0.001\n",
        "WINDOW_SIZE=2048 #portion u look at when transforming audio, a power of 2 as value\n",
        "HOP_SIZE=441 #10 milisec jump in time, default hop length is 512 #quarter hop, overlap four dif windows\n",
        "\n",
        "\n",
        "#NOISE_FACTOR=np.random.rand(1)*0.05\n",
        "\n",
        "def audiomentations(audio, noise_chance, pitch_chance, speed_chance, noise_factor, sample_rate):\n",
        "    if np.random.rand(1)<noise_chance:\n",
        "      noise_scale=np.random.rand(1)*noise_factor\n",
        "      audio = (audio +noise_scale* np.random.normal(size=audio.shape[0]))\n",
        "    #ideally audio starts at numpy, every operation in audiomentations is numpy operation (librosa built on numpy)\n",
        "\n",
        "    \n",
        "    if np.random.rand(1)<pitch_chance:\n",
        "      #n_steps=0\n",
        "      n_steps=np.random.randint(-5,6)\n",
        "      audio = (librosa.effects.pitch_shift(audio, sample_rate, n_steps))\n",
        "    \n",
        "      \n",
        "    if np.random.rand(1)<speed_chance:\n",
        "      #speed_rate=1 # 1 to 1.5\n",
        "      #speed_rate=(np.random.randint(-2,6)/10)+1 #0.8 to 1.5\n",
        "      speed_rate=(np.random.randint(0,6)/10)+1 #1 to 1.5\n",
        "      audio = (librosa.effects.time_stretch(audio, speed_rate))\n",
        "    \n",
        "\n",
        "    return audio\n",
        "    #augmentation only on training set, not validation \n",
        "   \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5K91QzJPNBe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpnKY3XZfFJn",
        "execution": {
          "iopub.status.busy": "2021-11-18T21:42:05.461739Z",
          "iopub.execute_input": "2021-11-18T21:42:05.462286Z",
          "iopub.status.idle": "2021-11-18T21:42:05.510501Z",
          "shell.execute_reply.started": "2021-11-18T21:42:05.462244Z",
          "shell.execute_reply": "2021-11-18T21:42:05.509695Z"
        },
        "trusted": true
      },
      "source": [
        "\n",
        "\n",
        "CSV_FILE=\"UrbanSound8K.csv\" \n",
        "data = pd.read_csv(CSV_FILE)\n",
        "data.head(20)\n",
        "\n",
        "print(list(data['classID'].unique()))\n",
        "ids = []\n",
        "temp=list(data['class'].unique())\n",
        "for name in temp:\n",
        "  ids.append(list(data[data['class']==name]['classID'].unique()))\n",
        "  \n",
        "print((ids))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_list=data[\"class\"].unique().tolist()\n",
        "class_name_dict={}\n",
        "name_to_id={}\n",
        "for name in data_list:\n",
        "  #print(name)\n",
        "  '''\n",
        "  class_num=data[data[\"class\"]==name][\"classID\"].iloc[0]\n",
        "  class_name_dict[class_num]=name\n",
        "  '''\n",
        "for name in data_list:\n",
        "  class_num=data[data[\"class\"]==name][\"classID\"].iloc[0]\n",
        "  class_name_dict[class_num]=name\n",
        "  name_to_id[name]=class_num\n",
        "\n",
        "\n",
        "print(name_to_id)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GbcKpu-O14N0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if CSV_FILE==\"UrbanSound8K.csv\":\n",
        "\n",
        "  print(data['class'].value_counts())\n",
        "else:\n",
        "  #data['class']=data['category']\n",
        "  data=data.rename(columns={\"category\": \"class\"})\n",
        "  print(data['class'].value_counts())\n"
      ],
      "metadata": {
        "id": "b6f2PW_hZDrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##data cleaning\n",
        "###removing files from dataset that include only static noise, or has multiple classes of sounds in file \n",
        "ex: 13577-3-0-2.wav (drilling and dog bark inside)\n",
        "\n",
        "###bad files\n",
        "relabel\n",
        "- 132016-9-0-16.wav (predicted=jackhammer, actual=streetmusic) num=10\n",
        "- 180937-7-3-33.wav(dril, jackhammer) num=24\n",
        "\n",
        "\n",
        "this one remove bc two classes:\n",
        "- 13577-3-0-2.wav(dril bark) 32\n",
        "- 43802-1-2-0.wav(siren, car horn) 27\n",
        "- 180937-4-2-1.wav (predicted=jackhammer, actuak=drilling) num= 3\n",
        "= 164797-2-0-47.wav (predicted=air dontiiona, actual =children( num=4\n",
        "- 35548-9-0-23.wav (children, street) num=6\n",
        "- 180937-4-1-30.wav(jackhammer drilling) num=18\n",
        "\n",
        "remove bc no classes:\n",
        "- 74677-0-0-0.wav(drilling air conditioner) num=23\n",
        "- 51022-3-30-4.wav (predicted=air conditioner, actual=dog bark)num=5\n",
        "- 177592-5-0-2.wav (air conditioner, engine idling)num=12"
      ],
      "metadata": {
        "id": "2tZHRlMdEFQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filestobedeleted= [\"7965-3-22-0.wav\",\"186336-9-0-4.wav\",\"177592-5-0-2.wav\",\"59037-2-1-2.wav\", \"13577-3-0-2.wav\",\"43802-1-2-0.wav\",\"180937-4-2-1.wav\",\"35548-9-0-23.wav\",\"180937-4-1-30.wav\",\"74677-0-0-0.wav\",\"51022-3-30-4.wav\",\"177592-5-0-2.wav\"]\n",
        "data=data[~data[\"slice_file_name\"].isin(filestobedeleted)]\n",
        "print(data['class'].value_counts())\n",
        "\n"
      ],
      "metadata": {
        "id": "KoixAwq4dkIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filestoberelabeled= {\"13577-3-0-2.wav\":'drilling',\"4918-3-0-0.wav\":'dog_bark',\"132016-9-0-16.wav\":'street_music',\"180937-7-3-33.wav\":'jackhammer'}\n",
        "\n",
        "for key, value in filestoberelabeled.items():\n",
        "  data.loc[data['slice_file_name']==key,'class']=value\n",
        "  data.loc[data['slice_file_name']==key,'classID']=name_to_id[value]\n",
        "\n",
        "print(data['class'].value_counts())\n",
        "\n",
        "#dont run this if havent figured out what files to delete yet\n"
      ],
      "metadata": {
        "id": "NW8Ju0R2hB3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLFZ3XU_fFJq",
        "execution": {
          "iopub.status.busy": "2021-11-18T21:42:39.960833Z",
          "iopub.execute_input": "2021-11-18T21:42:39.961202Z",
          "iopub.status.idle": "2021-11-18T21:42:39.972459Z",
          "shell.execute_reply.started": "2021-11-18T21:42:39.961148Z",
          "shell.execute_reply": "2021-11-18T21:42:39.971439Z"
        },
        "trusted": true
      },
      "source": [
        "  \n",
        "\n",
        "\n",
        "\n",
        "def features_extract(file_name): #load in files as non augmented, augment later\n",
        "  \n",
        "\n",
        "    #audio_binary = tf.io.read_file(file_path)\n",
        "    #audio, sample_rate = tf.audio.decode_wav(tf.io.read_file(file_name), desired_samples = SAMPLE_LENGTH)\n",
        "    audio, sample_rate = librosa.load(file_name, sr=SAMPLING_FREQUENCY, res_type = 'kaiser_fast') #take in file name and load file, output audio sample rate\n",
        "    audio=audiomentations(audio, NOISE_CHANCE, PITCH_CHANCE, SPEED_CHANCE, NOISE_FACTOR, sample_rate)\n",
        "    '''\n",
        "    if (sample_rate!=SAMPLING_FREQUENCY):\n",
        "      audio = librosa.resample(audio, librosa.core.get_samplerate(file_name), SAMPLING_FREQUENCY)\n",
        "    '''\n",
        "    #assert(sample_rate==SAMPLING_FREQUENCY), \"sampling freq isnt 44,100 for this loaded file\" #throws error\n",
        "    #def {audio, sample_rate} #dictionary\n",
        "    #audio=audio[SAMPLE_LENGTH:numOfZeros]\n",
        "\n",
        "    #random augmentations, mixtures of augmentions\n",
        "    \n",
        "    #if (CSV_FILE=\"esc50.csv\") SAMPLE_LENGTH=220500\n",
        "    if audio.shape[0]==SAMPLE_LENGTH:\n",
        "        pass\n",
        "    elif audio.shape[0]>SAMPLE_LENGTH:\n",
        "        audio=audio[0:SAMPLE_LENGTH]\n",
        "    elif audio.shape[0]<SAMPLE_LENGTH: #repeat the audio from beginning if less than 4 sec\n",
        "        audio = np.resize(audio,SAMPLE_LENGTH)\n",
        "    #audio_tf=tf.convert_to_tensor(audio, dtype=tf.float32)\n",
        "\n",
        "    #padding audio with zeroes make sure audio length is the same\n",
        "    '''\n",
        "    if audio.shape[0]==SAMPLE_LENGTH:\n",
        "        pass\n",
        "    elif audio.shape[0]>SAMPLE_LENGTH:\n",
        "        audio=audio[0:SAMPLE_LENGTH]\n",
        "    elif audio.shape[0]<SAMPLE_LENGTH:\n",
        "        numOfZeros=SAMPLE_LENGTH-audio.shape[0]\n",
        "        zerosArray=np.zeros(numOfZeros)\n",
        "        audio=np.concatenate((audio, zerosArray), axis=0)\n",
        "    audio_tf=tf.convert_to_tensor(audio, dtype=tf.float32)\n",
        "    #change audiio into mfccs\n",
        "    '''\n",
        "    ''' \n",
        "    mfccs_features = librosa.feature.mfcc(y = audio, sr = sample_rate, n_mfcc = 100, hop_length=HOP_SIZE, n_fft=WINDOW_SIZE) #sound=wave like sin curves\n",
        "    mfccs_scaled_features = np.mean(mfccs_features.T,axis = 0)\n",
        "    \n",
        "    '''\n",
        "    #spectrograms+mfccs= more complex concatenate these\n",
        "    #librosa.feature.melspectrogram(y=audio, sr=sample_rate, )\n",
        "\n",
        "    \n",
        " \n",
        "   \n",
        "    #melspectrogram includes fourier transforms, doing the math for you\n",
        "    '''\n",
        "    spectr= librosa.feature.melspectrogram(audio, sr=sample_rate, n_fft = WINDOW_SIZE, hop_length = HOP_SIZE, center = True, n_mels = N_MELS, fmax=None)\n",
        "    spectr_db = librosa.power_to_db(spectr, ref = np.max)\n",
        "    concat=np.concatenate((spectr_db, mfccs_features), axis=0 )\n",
        "    concat=np.expand_dims(concat, axis=-1)\n",
        "    #axis=-1 the last axis, place new dimention in last spot\n",
        "    #np normalize? concatenate in this function? for cnn training but skip normalize for now\n",
        "    return concat\n",
        "    \n",
        "    '''\n",
        "    return audio\n",
        "    #, sample_rate\n",
        " \n",
        "#   shape of these things like spectr_db\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Processing the Dataset using a convenient method"
      ],
      "metadata": {
        "id": "P-hh5nEtWgeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_dataset_path = '' #empty because we are already at fold 1, fold 2, etc\n",
        "extracted_features = [] #empty because its a list that we are appending to\n",
        "if (CSV_FILE==\"esc50.csv\"): #esc50\n",
        "  for index_num,row in tqdm(data.iterrows()):\n",
        "      file_name = os.path.join(os.path.abspath(audio_dataset_path),'audio','audio','44100',str(row[\"filename\"]))\n",
        "      final_class_labels = row[\"class\"]\n",
        "      \n",
        "      #srtest=librosa.core.get_samplerate(file_name)\n",
        "      \n",
        "      wav = features_extract(file_name) #data_ is a tuple\n",
        "      #, sr\n",
        "      #epoch level augmentations better. meansthere are augs applied when running model , every epoch will have a slightly dif representation, transformed version of the audio, dep on probability of aug appearing\n",
        "      #no aug, model will start memorizing the same set of audio it sees so many times. \n",
        "      #make model generalze betterand learn, if aug at loading level, just change original file to be slightly changed , but still give 3740 files to model 50 times (epoch=50 for ex)\n",
        "      extracted_features.append([wav, final_class_labels])\n",
        "else: #urbansound8k \n",
        "  for index_num,row in tqdm(data.iterrows()):\n",
        "      file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
        "      final_class_labels = row[\"class\"]\n",
        "       \n",
        "      fold_nums = row[\"fold\"]\n",
        "      wav = features_extract(file_name) #data_ is a tuple\n",
        "      #, sr\n",
        "      #epoch level augmentations better. meansthere are augs applied when running model , every epoch will have a slightly dif representation, transformed version of the audio, dep on probability of aug appearing\n",
        "      #no aug, model will start memorizing the same set of audio it sees so many times. \n",
        "      #make model generalze betterand learn, if aug at loading level, just change original file to be slightly changed , but still give 3740 files to model 50 times (epoch=50 for ex)\n",
        "      extracted_features.append([wav, final_class_labels, fold_nums])"
      ],
      "metadata": {
        "id": "FEzqS_YHSvgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SPLIT DATASET INTO AUDIO FILES AND CATEGORIES & ONE HOT ENCODING "
      ],
      "metadata": {
        "id": "NRo_7y-j5X53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_df=pd.DataFrame(extracted_features,columns=['audio', 'class', 'fold_num']) #'sr',\n",
        "features_df.head()\n",
        "features_df['audio'] "
      ],
      "metadata": {
        "id": "CkUCyIJyXcWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped=features_df.groupby(by=[\"fold_num\",'class'])\n",
        "\n",
        "\n",
        "for idx in range(1,11):\n",
        "  print(\"fold idx\")\n",
        "  print( features_df[features_df[\"fold_num\"]==idx]['class'].value_counts())"
      ],
      "metadata": {
        "id": "hiUSQYXdGt-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = features_df['audio']\n",
        "y_text = features_df['class']"
      ],
      "metadata": {
        "id": "K5jx8YvHXn8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labelencoder = LabelEncoder()\n",
        "y = to_categorical(labelencoder.fit_transform(y_text))\n",
        "\n",
        "#features_df['class_encode']=[r for r in y]\n",
        "#print(features_df.head)\n",
        "'''\n",
        "#Label Encoding  one hot encoding\n",
        "#using label encoder to get back the class name using inverse label encoder \n",
        "labelencoder = LabelEncoder()\n",
        "y = to_categorical(labelencoder.fit_transform(y_text)) # tranform class label. fit transform can only get a list \n",
        "#.fittransform(y) was saved to y before. change list of 3740 words into one hot encoded array (3740,10) \n",
        "#saved back 9into y, now give array of 3740,10. thats why change into y_text\n",
        "#y=tf.convert_to_tensor(y)\n",
        "'''"
      ],
      "metadata": {
        "id": "BI_Zmyv6XuIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SPLIT DATASET INTO X AND y "
      ],
      "metadata": {
        "id": "gzPIltNe5Owa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1, stratify = y)"
      ],
      "metadata": {
        "id": "kqY7BHFiXy_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FOLD_NUM=1 #ONLY CHANGE THIS ONE IN K CROSS VALIDATION\n",
        "\n",
        "#X_test=features_df[features_df[\"fold_num\"]==FOLD_NUM]['audio']\n",
        "##X_train=features_df[features_df[\"fold_num\"]!=FOLD_NUM]['audio']\n",
        "\n",
        "##y_train=features_df[features_df[\"fold_num\"]!=FOLD_NUM]['class_encode'].to_numpy()\n",
        "#y_test=features_df[features_df[\"fold_num\"]==FOLD_NUM]['class_encode'].to_numpy()\n"
      ],
      "metadata": {
        "id": "fbuRUN2iWWrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.to_numpy()\n",
        "X_test = X_test.to_numpy()\n",
        "\n",
        "\n",
        "#this step converts dataframes to numpy"
      ],
      "metadata": {
        "id": "tzUGEhgLl0zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type (X_train)\n"
      ],
      "metadata": {
        "id": "xgEAsMUclsyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tf.stack( [aud for aud in X_train] )\n",
        "#print(type(X_train_tf))\n",
        "#print(X_train_tf[0])\n",
        "#X_train_tf.shape\n",
        "\n",
        "X_test= tf.stack( [aud for aud in X_test] )"
      ],
      "metadata": {
        "id": "hZcqNsPWX344"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soXfjY_3fFJ-"
      },
      "source": [
        "## RALCT MODEL ARCHITECTURE and AUDIOMENTATIONS CLASS (rerun from here if model.fit not working)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km42_PXQfFKA",
        "execution": {
          "iopub.status.busy": "2021-11-11T23:42:39.174619Z",
          "iopub.execute_input": "2021-11-11T23:42:39.175177Z",
          "iopub.status.idle": "2021-11-11T23:42:39.198861Z",
          "shell.execute_reply.started": "2021-11-11T23:42:39.175138Z",
          "shell.execute_reply": "2021-11-11T23:42:39.197967Z"
        },
        "trusted": true
      },
      "source": [
        "#labels = y.shape[1] # total target variable or class variable\n",
        "#input_size = X.shape[1] # total feature value like here n_mfcc value \n",
        "#print(f\"number of total class label '{labels}'\")\n",
        "#print(f\"number of features used '{input_size}' \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#POSITIONAL ENCODING CLASS & LOG MEL SPECTROGRAM CLASS "
      ],
      "metadata": {
        "id": "pJCG28GwJcgR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-18T21:43:19.630175Z",
          "iopub.execute_input": "2021-11-18T21:43:19.630653Z",
          "iopub.status.idle": "2021-11-18T21:43:19.645899Z",
          "shell.execute_reply.started": "2021-11-18T21:43:19.630611Z",
          "shell.execute_reply": "2021-11-18T21:43:19.645239Z"
        },
        "trusted": true,
        "id": "W2AKFP86BU5o"
      },
      "source": [
        "\n",
        "#customized training loop?\n",
        "#frequency, time masking, tensorflow version\n",
        "\n",
        "seconds=5\n",
        "\n",
        "SAMPLING_FREQUENCY=44_100\n",
        "WINDOW_SIZE=2048 #portion u look at when transforming audio, a power of 2 as value\n",
        "HOP_SIZE=441 #10 milisec jump in time, default hop length is 512 #quarter hop, overlap four dif windows\n",
        "SAMPLE_LENGTH=seconds*SAMPLING_FREQUENCY #most files are 4 secs long\n",
        "WINDOW_SIZE=2048 #portion u look at when transforming audio, a power of 2 as value\n",
        "HOP_SIZE=441 #10 milisec jump in time, default hop length is 512\n",
        "#quarter hop, overlap four dif windows\n",
        "\n",
        "\n",
        "N_MELS=128\n",
        "\n",
        "\n",
        "      \n",
        "      \n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "   def __init__(self, first_shape=55, sec_shape=4, third_shape=32, **kwargs):\n",
        "        super(PositionalEncoding, self).__init__(**kwargs) \n",
        "        \n",
        "        self.pe = tf.Variable(initial_value = tf.random.normal(shape=(first_shape, sec_shape, third_shape), mean=0.0, stddev=0.2),\n",
        "                              trainable = True, name = 'Positional_Encoding_weights') \n",
        "   def call(self, sequence):\n",
        "        \n",
        "        return tf.math.add(sequence, self.pe) #track gradients, back propagation, assign correct weights, train weights properly\n",
        "\n",
        "class LogMelSpectrogram(tf.keras.layers.Layer):\n",
        "    \"\"\"Compute log-magnitude mel-scaled spectrograms.\"\"\"\n",
        "\n",
        "    def __init__(self, sample_rate, fft_size, hop_size, n_mels,\n",
        "                 f_min=0.0, f_max=None, **kwargs):\n",
        "        super(LogMelSpectrogram, self).__init__(**kwargs)\n",
        "        self.sample_rate = sample_rate\n",
        "        self.fft_size = fft_size\n",
        "        self.hop_size = hop_size\n",
        "        self.n_mels = n_mels\n",
        "        self.f_min = f_min\n",
        "        self.f_max = f_max if f_max else sample_rate / 2\n",
        "        self.mel_filterbank = tf.signal.linear_to_mel_weight_matrix(\n",
        "            num_mel_bins=self.n_mels,\n",
        "            num_spectrogram_bins=fft_size // 2 + 1,\n",
        "            sample_rate=self.sample_rate,\n",
        "            lower_edge_hertz=self.f_min,\n",
        "            upper_edge_hertz=self.f_max)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.non_trainable_weights.append(self.mel_filterbank)\n",
        "        super(LogMelSpectrogram, self).build(input_shape)\n",
        "\n",
        "    def call(self, waveforms):\n",
        "        \"\"\"Forward pass.\n",
        "        Parameters\n",
        "        ----------\n",
        "        waveforms : tf.Tensor, shape = (None, n_samples)\n",
        "            A Batch of mono waveforms.\n",
        "        Returns\n",
        "        -------\n",
        "        log_mel_spectrograms : (tf.Tensor), shape = (None, time, freq, ch)\n",
        "            The corresponding batch of log-mel-spectrograms\n",
        "        \"\"\"\n",
        "        def _tf_log10(x):\n",
        "            numerator = tf.math.log(x)\n",
        "            denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))\n",
        "            return numerator / denominator\n",
        "\n",
        "        def power_to_db(magnitude, amin=1e-16, top_db=80.0):\n",
        "            \"\"\"\n",
        "            https://librosa.github.io/librosa/generated/librosa.core.power_to_db.html\n",
        "            \"\"\"\n",
        "            ref_value = tf.reduce_max(magnitude)\n",
        "            log_spec = 10.0 * _tf_log10(tf.maximum(amin, magnitude))\n",
        "            log_spec -= 10.0 * _tf_log10(tf.maximum(amin, ref_value))\n",
        "            log_spec = tf.maximum(log_spec, tf.reduce_max(log_spec) - top_db)\n",
        "\n",
        "            return log_spec\n",
        "\n",
        "        spectrograms = tf.signal.stft(waveforms,\n",
        "                                      frame_length=self.fft_size,\n",
        "                                      frame_step=self.hop_size,\n",
        "                                      pad_end=False)\n",
        "\n",
        "        magnitude_spectrograms = tf.abs(spectrograms)\n",
        "\n",
        "        mel_spectrograms = tf.matmul(tf.square(magnitude_spectrograms),\n",
        "                                     self.mel_filterbank)\n",
        "\n",
        "        log_mel_spectrograms = power_to_db(mel_spectrograms)\n",
        "\n",
        "        # add channel dimension\n",
        "        log_mel_spectrograms = tf.expand_dims(log_mel_spectrograms, 3)\n",
        "\n",
        "        return log_mel_spectrograms\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'fft_size': self.fft_size,\n",
        "            'hop_size': self.hop_size,\n",
        "            'n_mels': self.n_mels,\n",
        "            'sample_rate': self.sample_rate,\n",
        "            'f_min': self.f_min,\n",
        "            'f_max': self.f_max,\n",
        "        }\n",
        "        config.update(super(LogMelSpectrogram, self).get_config())\n",
        "\n",
        "        return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4cxRfxy7QzG"
      },
      "source": [
        "## Transformers +CNN  Model for Audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3clhn3Hi8ivI"
      },
      "source": [
        "encoding + decoding = input and output language or audio or music \n",
        "\n",
        "only encoding- input audio/language and output a classification\n",
        "\n",
        "autoencoder squish into lower dimensions- tasked with recreate exact image from lower dimensional space\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRANSFORMER ENCODER CLASS "
      ],
      "metadata": {
        "id": "8fIrhutd6pIU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdCLf3T24pHu"
      },
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    #DID CONVOLUTIONS THEN ATTENTION TO DECREASE TIME STEP INFORMATION INTO A LOWER NUMBER SO ATTENTION LAYER CAN DEAL WITH SMALLER NUMBERS AND NOT GET OUT OF MEMORY\n",
        "    #ATTENTION COMBINES FEATURES OF DIF TIME STEPS TOGETHER. \n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs) #dif from batch normalization, normalize at every single computation (normalize x - xmean all divided by x standard deviation + epsilon to avoid divide by 0)\n",
        "    x = layers.MultiHeadAttention( #dot product with itself produce numbers, get important numbers, split up into dif heads, each head attends to a dif important part of your data\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout #size of each head, how many heads/filters/dif ways to attend to pr undestand or describe your data.  head size= kernel size, how many dimensions n your multi head attention, feature size\n",
        "        #Conv2D(32, 5x5 kernals) \n",
        "    )(x, x) #needs this bc this is self attention attend to itself, understand itself\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs #residual connection, combined with other features, lose some info alow he way. the residual connection allow model to decide if previous features are  more less or equal important to features it has created\n",
        "    #multihead attention on the inputs, got new representation of the inputs, residual connection allow original input to come back and be important still, add their values together\n",
        "    #residuals shortcut stuff, can access info u used to know, model will figure out the most improtant parts\n",
        "\n",
        "    # Feed Forward Part, activation part after do feature maps part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    #Dense(128, activation='relu')(y) \n",
        "    x = layers.Dense(ff_dim, activation='relu')(x) #num units in dense layer is ff_dim\n",
        "    #x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(units=inputs.shape[-1])(x) #throw dimensions back into what we had before - can do residual layer again\n",
        "    #x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res #encoders stacked on top of each other"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL AUG CLASS\n",
        "##GRADIENT TAPE INSIDE THE CLASS \n",
        "\n"
      ],
      "metadata": {
        "id": "Y9JpCyKK68F1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelAug (keras.Model): #has all features and methods of keras models= parameter is still input and output\n",
        "    def train_step(self, data): #training loop can have augmentations \n",
        "        # Unpack the data. Its structure depends on your model and\n",
        "        # on what you pass to `fit()`.\n",
        "        x, y = data #unaugmented data\n",
        "        #print(type(x))\n",
        "        x=x.numpy()\n",
        "        #print(type(x))\n",
        "\n",
        "        if (x.shape[0]!=None) :\n",
        "          list_of_aug_audios=[] #python list of numpy arrays\n",
        "          for i in range(x.shape[0]):\n",
        "            audio=x[i]\n",
        "            #audio=audiomentations(audio, NOISE_CHANCE, PITCH_CHANCE, SPEED_CHANCE, NOISE_FACTOR, SAMPLING_FREQUENCY)\n",
        "            if audio.shape[0]==SAMPLE_LENGTH:\n",
        "              pass\n",
        "            elif audio.shape[0]>SAMPLE_LENGTH:\n",
        "              audio=audio[0:SAMPLE_LENGTH]\n",
        "            elif audio.shape[0]<SAMPLE_LENGTH: #repeat the audio from beginning if less than 4 sec\n",
        "              audio = np.resize(audio,SAMPLE_LENGTH)\n",
        "            #ensure all audio files have same length\n",
        "            list_of_aug_audios.append(audio)\n",
        "          x=np.stack(list_of_aug_audios, axis=0) #x is a 2d array (row dim=examples, y is indep number (44100*4))\n",
        "\n",
        "        #tf.convert_to_tensor()\n",
        "        with tf.GradientTape() as tape: #a gradient tape records all tensorflow operations , call the model, x gets transformed into spec/mfccs/concatenate, etc etc etc\n",
        "            y_pred = self(x, training=True)  # Forward pass\n",
        "            # Compute the loss value\n",
        "            # (the loss function is configured in `compile()`)\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "            #backpropgation used to figure out how weights should be changed - optimize the model's weights , main method to train a network\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {m.name: m.result() for m in self.metrics} # in this batch, step of training, got this much loss and accuracy "
      ],
      "metadata": {
        "id": "OO7gZjkKfVaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BUILDING THE RALCT MODEL & DETAILS OF ITS LAYERS "
      ],
      "metadata": {
        "id": "dt-seL7J7OhC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJXIuUxJ7yNw"
      },
      "source": [
        "\n",
        "\n",
        "def build_model(\n",
        "    n_classes,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks, #transformer encoders happen in a row num of blocks t9mes stack that many tras=nsformer blocks on top of each otehr\n",
        "    mlp_units, #do dense layers after [128,128,64] all dense layers\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,  sample_rate=SAMPLING_FREQUENCY, duration=seconds,\n",
        "              fft_size=WINDOW_SIZE, hop_size= HOP_SIZE, n_mels=N_MELS\n",
        "):\n",
        "\n",
        "    n_samples = sample_rate * duration\n",
        "    # Accept raw audio data as input\n",
        "    inputs = Input(shape=(n_samples,), name='input', dtype='float32')\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    # Process into log-mel-spectrograms. (This is your custom layer!)\n",
        "    spectro = LogMelSpectrogram(sample_rate, fft_size, hop_size, n_mels)(inputs) \n",
        "    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(spectro)\n",
        "    #0 is batch size, 1 is time, 2 is features(mel bins, what freq are in that section of time), stack for each time, stack mfccs\n",
        "  \n",
        "      \n",
        "      \n",
        "    # Normalize data (on frequency axis)\n",
        "    spectro_norm = BatchNormalization(axis=2)(spectro)\n",
        "    \n",
        "    mfccs_norm = BatchNormalization(axis=2)(mfccs)\n",
        "    #normalize spec and mfcc separately because they have dif numbers and units inherently\n",
        "    #batch norm means normalize between 0 to 1 \n",
        "    x = tf.concat([spectro_norm, mfccs_norm], axis=3, name='concat') #concatenate along/extends the feature dimention. add more features \n",
        "    # in that case, both the batch size and the time must be the same for both the spectros and mfccs\n",
        "    #axis 2 vs axis 3. axis 2 tells concat to align matrice array to feature dimension. first dim= time step. 2nd=features\n",
        "    #  3rd dimention is dif represent of features. red green blue dif info but represen same type\n",
        "    #benefit : 3rd axis = stack on the channel axis. the filter kernel can treat the mfcss and spect #s different\n",
        "    #\n",
        "    '''\n",
        "    xbranch1 = Conv2D(16, (3, 3), activation='relu', padding = 'same')(x) #16 is number of filters, channels/feature maps produced, 16 dif way describe audio\n",
        "    xbranch1 = BatchNormalization()(xbranch1)\n",
        "\n",
        "    xbranch2 = Conv2D(16, (5, 5), activation='relu', padding = 'same')(x) #(3,1) was because x and y have dif meanings for audio, same meanings for image. spectrogram- x dir means time, y dir is the features. \n",
        "    xbranch2 = BatchNormalization()(xbranch2)\n",
        "\n",
        "    xbranch3 = Conv2D(16, (7, 7), activation='relu', padding = 'same')(x)\n",
        "    xbranch3 = BatchNormalization()(xbranch3)\n",
        "\n",
        "    xbranch4 = Conv2D(16, (11, 11), activation='relu', padding = 'same')(x)\n",
        "    xbranch4 = BatchNormalization()(xbranch4)\n",
        "    '''\n",
        "\n",
        "    #concatenate as inception block \n",
        "    #x = tf.concat([xbranch1, xbranch2, xbranch3, xbranch4], axis=3, name='incep_concat') #sep concat so model can choose what kernel to do, 3x3 or 5x5 etc\n",
        "    #400x128 #+ add tog = destroying info #axis 0 batch 1 time 2 feature(in the spectrogram, or a freq in a time) 3 channel (mel and spec same dimension)\n",
        "    #gives model opportunity to say the other kernels are important, not constrict model to a specific filter like 5x5\n",
        "    #higher weight=more important\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    x = Conv2D(32, (5, 5), activation='relu', padding = 'same')(x) #16 is number of filters, channels/feature maps produced, 16 dif way describe audio\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPool2D((1, 3))(x) #y.shape 2 grab entire length of the features, take max of entire feature space. throw away all other features. lose features. dont want to do y.shape(2) in the max pool\n",
        "    #convolutions 2 channels deep, 3 by 3, thats becasue we have mfccs and spec\n",
        "    x = Dropout(0.1)(x)\n",
        "    \n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding = 'same')(x) #(3,1) was because x and y have dif meanings for audio, same meanings for image. spectrogram- x dir means time, y dir is the features. \n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPool2D(pool_size=(3, 3))(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    \n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding = 'same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPool2D(pool_size=(3, 3))(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    \n",
        "   \n",
        "   \n",
        "\n",
        "    first_shape=55 #if (CSV_FILE==\"esc50.csv\") else 44\n",
        "      \n",
        "    \n",
        "     \n",
        "    \n",
        "    x = PositionalEncoding()(x)\n",
        "   \n",
        "    #add location\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   \n",
        "    #inputs = keras.Input(shape=input_shape)\n",
        "    #x = inputs\n",
        "    # n_samples = sample_rate * duration\n",
        "    # Accept raw audio data as input\n",
        "  \n",
        "    #inputs_aug=Audiomentations(NOISE_CHANCE, PITCH_CHANCE, SPEED_CHANCE, SAMPLING_FREQUENCY)(inputs)\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    # Process into log-mel-spectrograms\n",
        "\n",
        "    #0 is batch size, 1 is time, 2 is features(mel bins, what freq are in that section of time), stack for each time, stack mfccs\n",
        "  \n",
        "    #decrease time steps or else memory too big when multiplying together\n",
        "    #average time not features, shrink time dimension by factor of 5 not feature dimensions tho\n",
        "    # Normalize data (on frequency axis)\n",
        "\n",
        "    #normalize spec and mfcc separately because they have dif numbers and units inherently\n",
        "    #batch norm means normalize between 0 to 1 \n",
        "    #axis  1 time, 2 features\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    #x = layers.GlobalAveragePooling2D(data_format=\"channels_first\")(x) #flattening or averaging values, stride along 1 dimension, averages along time or features , treat all features same 1D. only have a 2D input- time step and feature, no channels\n",
        "    #globalpool lose too much data\n",
        "    x = Flatten()(x) \n",
        "    for dim in mlp_units: #first hidden layer has first dims, 100 in second hidden layer, for example, etc\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "\n",
        "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x) #rip off last layer and take a pretrained model on urbansound8k and tarain on esc50.\n",
        "    return ModelAug(inputs, outputs) #very first and last values- inputs and outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_test_tf.shape"
      ],
      "metadata": {
        "id": "rl9hQZ00odRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RUNNING AND TRAINING THE MODEL\n",
        "### REMEMBER\n",
        "UrbanSound8K n_classes=10! urbansound8K contains 44100 and 22050 hz audio files\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CM5Y5UwGeUWa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9eBAIvd77ej"
      },
      "source": [
        "#input_shape = x_train.shape[1:]\n",
        "\n",
        "model = build_model(\n",
        "    #n_classes=50,\n",
        "    n_classes=10,\n",
        "    head_size=64, #before was 32\n",
        "    num_heads=4,\n",
        "    ff_dim=32,\n",
        "    num_transformer_blocks=2,\n",
        "    mlp_units=[64], #before was 128\n",
        "    mlp_dropout=0.1, #before was 0.4\n",
        "    dropout=0.25,\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=120\n",
        "BATCH_SIZE=32\n",
        "total_train = X_train.shape[0]\n",
        "STEPS=(total_train/BATCH_SIZE)*EPOCHS\n",
        "initial_learning_rate=1e-3 #was 1e-3 then 1e-4 before\n",
        "lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate, STEPS)\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=lr_decayed_fn),\n",
        "    metrics=['accuracy'],  run_eagerly=True\n",
        ")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "EMqmZF6xDatQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_test=X_test.to_numpy()\n",
        "#type(X_test)"
      ],
      "metadata": {
        "id": "uim7W0GBdUQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train Model (try rerun from the model definition)"
      ],
      "metadata": {
        "id": "8q_DZ467p2Hl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dki2yOu2UeNY"
      },
      "source": [
        "#callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
        "\n",
        "model.fit(\n",
        "    X_train,\n",
        "    #X_train,\n",
        "    y_train,\n",
        "    \n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    #callbacks=callbacks,\n",
        "    validation_data = (X_test, y_test), verbose = 1)\n",
        "    #validation_data = (X_test, y_test), verbose = 1)\n",
        "  \n",
        "#validation_split=0.2,\n",
        "\n",
        "#model.evaluate(X_test_tf, y_test, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for layer in model.layers:\n",
        "  #layer.trainable=True"
      ],
      "metadata": {
        "id": "n8KT3wFqFBVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVING MODEL TO GOOGLE DRIVE"
      ],
      "metadata": {
        "id": "DwgAPreX8Mew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "2qzQFruuaBkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#save to drive as h5 model tf model\n",
        "model_to_be_saved=\"trial_model\" #change this for every best accurate model run and trained\n",
        "# Save the entire model as a SavedModel.\n",
        "#!mkdir -p saved_model\n",
        "#'/content/drive/MyDrive/Mask_RCNN'\n",
        "#model.save('/content/drive/MyDrive/saved_models/'+model_to_be_saved+'.h5')\n"
      ],
      "metadata": {
        "id": "RcnuQlFgZEHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CONVERTING MODEL TO TFLITE MODEL "
      ],
      "metadata": {
        "id": "4ojOv1h18TLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "model_to_be_converted=\"trial_model\"\n",
        "saved_model_dir='/content/drive/MyDrive/saved_models/'+ model_to_be_converted\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the tflite model.\n",
        "with open('/content/drive/MyDrive/saved_models/tfliteexports/'+ model_to_be_converted+'.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "'''"
      ],
      "metadata": {
        "id": "SilXm2AsxPv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "open(\"converted_model.tflite\", \"wb\").write(tflite_model)\n",
        "'''"
      ],
      "metadata": {
        "id": "VtSocJ4h0bCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "model_to_be_loaded=\"trial_model\"\n",
        "loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/saved_models/'+model_to_be_loaded)\n",
        "'''\n",
        "# Check its architecture\n",
        "#loaded_model.summary()"
      ],
      "metadata": {
        "id": "hk1VZ_Q5aQF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loaded_model=loaded_model.layers[:-1]"
      ],
      "metadata": {
        "id": "tKmAZGMq-UXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRINTING OUT LAYERS OF MODEL "
      ],
      "metadata": {
        "id": "6KjAaazE8dSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ONLY RUN THIS CELL. OUR MODEL IS NOT EXACTLY SEQUENTIAL. OURS HAS RESIDUAL CONNECTIONS, CONCATENATIONS\n",
        "'''\n",
        "base_output=loaded_model.layers[-2].output\n",
        "\n",
        "\n",
        "for layer in loaded_model.layers:\n",
        "    print(layer.name)\n",
        "\n",
        "\n",
        "\n",
        "new_output=layers.Dense(50, activation=\"softmax\")(base_output)\n",
        "esc50MODEL=tf.keras.models.Model(inputs=loaded_model.inputs, outputs=new_output)\n",
        "print(esc50MODEL.summary())\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "    if layer.name=='dense_6':\n",
        "    \n",
        "      layer._name = layer.name + str(\"_7\")\n",
        "      break\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "huElSbXy_cqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#first_layer=loaded_model.layers[0]\n",
        "#print(type(first_layer))\n",
        "\n",
        "#last_layer=loaded_model.layers[-1]\n",
        "#print(type(last_layer))\n",
        "\n"
      ],
      "metadata": {
        "id": "_SnpeWg3zdvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DONT RUN THIS IN TRANSFER LEARNING\n",
        "#for layer in esc50MODEL.layers[:-1]:\n",
        "  #layer.trainable=False"
      ],
      "metadata": {
        "id": "gM039rqk1wny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(esc50MODEL.trainable_variables)"
      ],
      "metadata": {
        "id": "U6uzNcU3gt2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model=esc50MODEL"
      ],
      "metadata": {
        "id": "OPuOyBMcgytU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3zpo_6cfFKD"
      },
      "source": [
        "## VISUALIZING RESULTS \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCTnUREIfFKE",
        "execution": {
          "iopub.status.busy": "2021-10-29T14:18:10.59939Z",
          "iopub.status.idle": "2021-10-29T14:18:10.600174Z",
          "shell.execute_reply.started": "2021-10-29T14:18:10.599814Z",
          "shell.execute_reply": "2021-10-29T14:18:10.59984Z"
        },
        "trusted": true
      },
      "source": [
        "#saving the model history\n",
        "loss = pd.DataFrame(model.history.history)\n",
        "\n",
        "#plotting the loss and accuracy \n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "plt.plot(loss[\"loss\"], label =\"Loss\")\n",
        "plt.plot(loss[\"val_loss\"], label = \"Validation_loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.plot(loss['accuracy'],label = \"Training Accuracy\")\n",
        "plt.plot(loss['val_accuracy'], label =\"Validation_ Accuracy \")\n",
        "plt.legend()\n",
        "plt.title(\"Training-Validation Accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V562arESfFKF"
      },
      "source": [
        "## Prediction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5FB6COZfFKG",
        "execution": {
          "iopub.status.busy": "2021-10-29T14:18:10.601877Z",
          "iopub.status.idle": "2021-10-29T14:18:10.602475Z",
          "shell.execute_reply.started": "2021-10-29T14:18:10.602229Z",
          "shell.execute_reply": "2021-10-29T14:18:10.602255Z"
        },
        "trusted": true
      },
      "source": [
        "prediction = model.predict(X_test)\n",
        "print(prediction.shape)\n",
        "\n",
        "# finding class with larget predicted probability using argmax of numpy \n",
        "y_pred = np.argmax(prediction, axis = 1)  # prediction using model \n",
        "y_test_orig = np.argmax(y_test, axis = 1) # original y_test\n",
        "print(y_pred)\n",
        "#argmax= look at each row and get index with highest number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "8KTel-kJroVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#target and category = esc50\n",
        "#classID and class = urbansound8k\n",
        "data_list=data[\"class\"].unique().tolist()\n",
        "class_name_dict={}\n",
        "for name in data_list:\n",
        "  #print(name)\n",
        "  '''\n",
        "  class_num=data[data[\"class\"]==name][\"classID\"].iloc[0]\n",
        "  class_name_dict[class_num]=name\n",
        "  '''\n",
        "for name in data_list:\n",
        "  class_num=data[data[\"class\"]==name][\"classID\"].iloc[0]\n",
        "  class_name_dict[class_num]=name\n",
        "  \n",
        "\n",
        "print(class_name_dict)\n"
      ],
      "metadata": {
        "id": "zu5DbId2eaqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKnKHKuafFKG",
        "execution": {
          "iopub.status.busy": "2021-10-29T14:18:10.604047Z",
          "iopub.status.idle": "2021-10-29T14:18:10.605009Z",
          "shell.execute_reply.started": "2021-10-29T14:18:10.604733Z",
          "shell.execute_reply": "2021-10-29T14:18:10.60477Z"
        },
        "trusted": true
      },
      "source": [
        "#Getting Class Label Name\n",
        "#class_name = np.array(features_df['class'].unique().tolist())\n",
        "'''\n",
        "for num in range(len(class_name_dict)):\n",
        "  print(class_name_dict[num])\n",
        "'''\n",
        "\n",
        "\n",
        "class_name=[class_name_dict[num] for num in range(len(class_name_dict)) ]\n",
        "print(class_name)\n",
        "\n",
        "          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVRkpAdHfFKH",
        "execution": {
          "iopub.status.busy": "2021-10-29T14:18:10.606424Z",
          "iopub.status.idle": "2021-10-29T14:18:10.606856Z",
          "shell.execute_reply.started": "2021-10-29T14:18:10.606624Z",
          "shell.execute_reply": "2021-10-29T14:18:10.606649Z"
        },
        "trusted": true
      },
      "source": [
        "#CLASSIFICATION REPORT CODE ACQUIRED FROM SONU KUMARI\n",
        "print(classification_report(y_test_orig, y_pred, target_names = class_name))\n",
        "#support means the total number of classes in test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZGeumYRfFKI",
        "execution": {
          "iopub.status.busy": "2021-10-29T14:18:10.608846Z",
          "iopub.status.idle": "2021-10-29T14:18:10.609703Z",
          "shell.execute_reply.started": "2021-10-29T14:18:10.609433Z",
          "shell.execute_reply": "2021-10-29T14:18:10.609456Z"
        },
        "trusted": true
      },
      "source": [
        "\n",
        "confusion_df = pd.DataFrame(confusion_matrix(y_test_orig, y_pred), columns = class_name, index = class_name)\n",
        "\n",
        "print(\"Confusion Matrix for Audio Dataset\")\n",
        "\n",
        "confusion_df\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
